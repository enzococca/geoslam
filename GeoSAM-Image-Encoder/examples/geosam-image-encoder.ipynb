{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enzococca/geoslam/blob/main/GeoSAM-Image-Encoder/examples/geosam-image-encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GeoSAM-Image-Encoder (Python package)\n",
        "\n",
        "[![PyPI Version](https://img.shields.io/pypi/v/GeoSAM-Image-Encoder)](https://pypi.org/project/GeoSAM-Image-Encoder)\n",
        "[![Downloads](https://static.pepy.tech/badge/GeoSAM-Image-Encoder)](https://pepy.tech/project/GeoSAM-Image-Encoder)\n",
        "\n",
        "\n",
        "This package is part of the [Geo-SAM](https://github.com/coolzhao/Geo-SAM) project and is a standalone Python package that does not depend on QGIS. This package allows you to **encode remote sensing images into features that can be recognized by Geo-SAM using a remote server**, such as ``Colab``, ``AWS``, ``Azure`` or your own ``HPC``."
      ],
      "metadata": {
        "id": "ofyKNeqP6Wj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "Installing `GeoSAM-Image-Encoder` may directly install the CPU version of `PyTorch`. Therefore, it is recommended to install the appropriate version of `PyTorch` before installing `GeoSAM-Image-Encoder` in your machine. You can install the corresponding version based on the official PyTorch website:\n",
        "<https://pytorch.org/get-started/locally/>\n",
        "\n",
        "After installing PyTorch, you can install `GeoSAM-Image-Encoder` via pip.\n"
      ],
      "metadata": {
        "id": "yd7SYS1o6NTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Colab, PyTorch is already built-in, so you can install it directly."
      ],
      "metadata": {
        "id": "dM0YGfCtsfUI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x_LOrmGt3Gkh",
        "outputId": "59b861d1-1a7e-419a-9550-4d7fb742c409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GeoSAM-Image-Encoder\n",
            "  Downloading GeoSAM_Image_Encoder-1.0.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from GeoSAM-Image-Encoder) (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from GeoSAM-Image-Encoder) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from GeoSAM-Image-Encoder) (75.1.0)\n",
            "Collecting torchgeo (from GeoSAM-Image-Encoder)\n",
            "  Downloading torchgeo-0.6.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting segment-anything (from GeoSAM-Image-Encoder)\n",
            "  Downloading segment_anything-1.0-py3-none-any.whl.metadata (487 bytes)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->GeoSAM-Image-Encoder) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->GeoSAM-Image-Encoder) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->GeoSAM-Image-Encoder) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->GeoSAM-Image-Encoder) (2025.1)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.11/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (0.8.0)\n",
            "Collecting fiona>=1.8.21 (from torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading fiona-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kornia>=0.7.3 (from torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading kornia-0.8.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lightly!=1.4.26,>=1.4.5 (from torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading lightly-1.5.18-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting lightning!=2.3.*,>=2 (from lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.5 in /usr/local/lib/python3.11/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (3.10.0)\n",
            "Requirement already satisfied: pillow>=8.4 in /usr/local/lib/python3.11/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (11.1.0)\n",
            "Requirement already satisfied: pyproj>=3.3 in /usr/local/lib/python3.11/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (3.7.0)\n",
            "Collecting rasterio!=1.4.0,!=1.4.1,!=1.4.2,>=1.3 (from torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting rtree>=1 (from torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading Rtree-1.3.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting segmentation-models-pytorch>=0.2 (from torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.11/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (2.0.6)\n",
            "Requirement already satisfied: timm>=0.4.12 in /usr/local/lib/python3.11/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (1.0.14)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (2.5.1+cu124)\n",
            "Collecting torchmetrics>=0.10 (from torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torchvision>=0.14 in /usr/local/lib/python3.11/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (0.20.1+cu124)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.21->torchgeo->GeoSAM-Image-Encoder) (25.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.21->torchgeo->GeoSAM-Image-Encoder) (2024.12.14)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.21->torchgeo->GeoSAM-Image-Encoder) (8.1.8)\n",
            "Collecting click-plugins>=1.0 (from fiona>=1.8.21->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting cligj>=0.5 (from fiona>=1.8.21->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting kornia_rs>=0.1.0 (from kornia>=0.7.3->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading kornia_rs-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kornia>=0.7.3->torchgeo->GeoSAM-Image-Encoder) (24.2)\n",
            "Collecting hydra-core>=1.0.0 (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting lightly_utils~=0.0.0 (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading lightly_utils-0.0.2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (2.32.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (1.17.0)\n",
            "Requirement already satisfied: pydantic>=1.10.5 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (2.10.6)\n",
            "Collecting pytorch_lightning>=1.0.4 (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (2.3.0)\n",
            "Collecting aenum>=3.1.11 (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading aenum-3.1.15-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (2024.10.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (4.12.2)\n",
            "Collecting jsonargparse<5.0,>=4.27.7 (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading jsonargparse-4.36.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting omegaconf<3.0,>=2.2.3 (from lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: rich<14.0,>=12.3.0 in /usr/local/lib/python3.11/dist-packages (from lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (13.9.4)\n",
            "Collecting tensorboardX<3.0,>=2.2 (from lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting bitsandbytes<1.0,>=0.44.0 (from lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->torchgeo->GeoSAM-Image-Encoder) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->torchgeo->GeoSAM-Image-Encoder) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->torchgeo->GeoSAM-Image-Encoder) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->torchgeo->GeoSAM-Image-Encoder) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->torchgeo->GeoSAM-Image-Encoder) (3.2.1)\n",
            "Collecting affine (from rasterio!=1.4.0,!=1.4.1,!=1.4.2,>=1.3->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting efficientnet-pytorch>=0.6.1 (from segmentation-models-pytorch>=0.2->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch>=0.2->torchgeo->GeoSAM-Image-Encoder) (0.27.1)\n",
            "Collecting pretrainedmodels>=0.7.1 (from segmentation-models-pytorch>=0.2->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm>=0.4.12->torchgeo->GeoSAM-Image-Encoder) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torchgeo->GeoSAM-Image-Encoder) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->torchgeo->GeoSAM-Image-Encoder) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (3.11.11)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.0.0->lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (0.16)\n",
            "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading typeshed_client-2.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting munch (from pretrainedmodels>=0.7.1->segmentation-models-pytorch>=0.2->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (3.10)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (2.18.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX<3.0,>=2.2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (4.25.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13->torchgeo->GeoSAM-Image-Encoder) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (1.18.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (0.1.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo->GeoSAM-Image-Encoder) (6.5.2)\n",
            "Downloading GeoSAM_Image_Encoder-1.0.4-py3-none-any.whl (27 kB)\n",
            "Downloading segment_anything-1.0-py3-none-any.whl (36 kB)\n",
            "Downloading torchgeo-0.6.2-py3-none-any.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.7/454.7 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fiona-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia-0.8.0-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightly-1.5.18-py3-none-any.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.0/849.0 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Rtree-1.3.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (543 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m543.2/543.2 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading segmentation_models_pytorch-0.4.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aenum-3.1.15-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonargparse-4.36.0-py3-none-any.whl (214 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.5/214.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n",
            "Downloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading typeshed_client-2.7.0-py3-none-any.whl (624 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Building wheels for collected packages: efficientnet-pytorch, antlr4-python3-runtime, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=e6d331f9614a100d12e54b714c3efc9dff3a3537415f88cc00a1981ec30e4dce\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/6f/9b/231a832f811ab6ebb1b32455b177ffc6b8b1cd8de19de70c09\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=3beee18dbd4aec60aacb88c98ca7cdbaa26c9dafd9c5261ab52f84a48312dc75\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=4c067738bd3c357889153f40e3fec90fde1c4f950ff3d42045a84d0d2c93e947\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/5b/96/fd94bc35962d7c6b699e8814db545155ac91d2b95785e1b035\n",
            "Successfully built efficientnet-pytorch antlr4-python3-runtime pretrainedmodels\n",
            "Installing collected packages: segment-anything, antlr4-python3-runtime, aenum, typeshed-client, tensorboardX, rtree, omegaconf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, munch, lightning-utilities, lightly_utils, kornia_rs, jsonargparse, cligj, click-plugins, affine, rasterio, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, fiona, nvidia-cusolver-cu12, torchmetrics, kornia, efficientnet-pytorch, bitsandbytes, pytorch_lightning, pretrainedmodels, segmentation-models-pytorch, lightning, lightly, torchgeo, GeoSAM-Image-Encoder\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed GeoSAM-Image-Encoder-1.0.4 aenum-3.1.15 affine-2.4.0 antlr4-python3-runtime-4.9.3 bitsandbytes-0.45.1 click-plugins-1.1.1 cligj-0.7.2 efficientnet-pytorch-0.7.1 fiona-1.10.1 hydra-core-1.3.2 jsonargparse-4.36.0 kornia-0.8.0 kornia_rs-0.1.8 lightly-1.5.18 lightly_utils-0.0.2 lightning-2.5.0.post0 lightning-utilities-0.12.0 munch-4.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 omegaconf-2.3.0 pretrainedmodels-0.7.4 pytorch_lightning-2.5.0.post0 rasterio-1.4.3 rtree-1.3.0 segment-anything-1.0 segmentation-models-pytorch-0.4.0 tensorboardX-2.6.2.2 torchgeo-0.6.2 torchmetrics-1.6.1 typeshed-client-2.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "d90cb34552234585859e78a078fec1b3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install GeoSAM-Image-Encoder\n",
        "# or\n",
        "# !pip install git+https://github.com/Fanchengyan/GeoSAM-Image-Encoder.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download example dataset and sam `vit_l` checkpoint"
      ],
      "metadata": {
        "id": "dM9ztEAm62Kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/coolzhao/Geo-SAM/main/rasters/beiluhe_google_img_201211_clip.tif\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
        "!wget https://raw.githubusercontent.com/coolzhao/Geo-SAM/main/GeoSAM-Image-Encoder/examples/data/setting.json"
      ],
      "metadata": {
        "id": "v32Lb6YW5FNU",
        "outputId": "77a95bec-50c8-46ed-c66e-f9fe7f176815",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-05 17:34:42--  https://raw.githubusercontent.com/coolzhao/Geo-SAM/main/rasters/beiluhe_google_img_201211_clip.tif\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17152742 (16M) [application/octet-stream]\n",
            "Saving to: ‘beiluhe_google_img_201211_clip.tif’\n",
            "\n",
            "beiluhe_google_img_ 100%[===================>]  16.36M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-02-05 17:34:44 (346 MB/s) - ‘beiluhe_google_img_201211_clip.tif’ saved [17152742/17152742]\n",
            "\n",
            "--2025-02-05 17:34:45--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 108.157.254.124, 108.157.254.121, 108.157.254.102, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|108.157.254.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1249524607 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_l_0b3195.pth’\n",
            "\n",
            "sam_vit_l_0b3195.pt 100%[===================>]   1.16G   130MB/s    in 7.1s    \n",
            "\n",
            "2025-02-05 17:34:52 (168 MB/s) - ‘sam_vit_l_0b3195.pth’ saved [1249524607/1249524607]\n",
            "\n",
            "--2025-02-05 17:34:52--  https://raw.githubusercontent.com/coolzhao/Geo-SAM/main/GeoSAM-Image-Encoder/examples/data/setting.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 494 [text/plain]\n",
            "Saving to: ‘setting.json’\n",
            "\n",
            "setting.json        100%[===================>]     494  --.-KB/s    in 0s      \n",
            "\n",
            "2025-02-05 17:34:53 (35.5 MB/s) - ‘setting.json’ saved [494/494]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage\n",
        "\n",
        "There are **two ways** to use GeoSAM-Image-Encoder. You can call it in Python or Terminal. We recommend using Python interface directly which will have greater flexibility."
      ],
      "metadata": {
        "id": "ILKiN60dXhQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Python\n",
        "\n",
        "After install GeoSAM-Image-Encoder, you can import it using `geosam`"
      ],
      "metadata": {
        "id": "96Dof82l31rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geosam\n",
        "from geosam import ImageEncoder"
      ],
      "metadata": {
        "id": "Z0K8RQV63H_v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if gpu available\n",
        "geosam.gpu_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwvXGZLYS5LZ",
        "outputId": "0529fcdc-0009-4f4e-f71f-ebd9f3d24f3a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run by specify parameters directly\n",
        "\n",
        "If you want to specify the parameters directly, you can run it like this:"
      ],
      "metadata": {
        "id": "gb03VNJe4O2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = '/content/sam_vit_l_0b3195.pth'\n",
        "image_path = '/content/beiluhe_google_img_201211_clip.tif'\n",
        "feature_dir = './'\n",
        "\n",
        "## init ImageEncoder\n",
        "img_encoder = ImageEncoder(checkpoint_path)\n",
        "## encode image\n",
        "img_encoder.encode_image(image_path,feature_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNwD3v3D8RD1",
        "outputId": "d627c917-1736-442c-c698-61199e762c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing SAM model...\n",
            "\n",
            "\n",
            "----------------------------------------------\n",
            "     Start encoding image to SAM features\n",
            "----------------------------------------------\n",
            "\n",
            "Input Parameters:\n",
            "----------------------------------------------\n",
            " Input data value range to be rescaled: [0, 255] (automatically set based on min-max value of input image inside the processing extent.)\n",
            " Image path: /content/beiluhe_google_img_201211_clip.tif\n",
            " Bands selected: ['1', '2', '3']\n",
            " Target resolution: 0.9999395530145561\n",
            " Processing extent: [471407.9709, 3882162.2353, 473331.8546, 3884389.1008]\n",
            " Processing image size: (width 1924, height 2227)\n",
            "----------------------------------------------\n",
            "\n",
            "\n",
            "RasterDataset info \n",
            "----------------------------------------------\n",
            " filename_glob: beiluhe_google_img_201211_clip.tif, \n",
            " all bands: ['1', '2', '3', '4'], \n",
            " input bands: ['1', '2', '3'], \n",
            " resolution: 0.9999395530145561, \n",
            " bounds: [471407.9709, 473331.8546571067, 3882162.2353493366, 3884389.1008, 0.0, 9.223372036854776e+18], \n",
            " num: 1\n",
            "----------------------------------------------\n",
            "\n",
            "----------------------------------------------\n",
            " SAM model initialized. \n",
            "  SAM model type:  vit_l\n",
            " Device type: cuda:0\n",
            " Patch size: (1024, 1024) \n",
            " Batch size: 1\n",
            " Patch sample num: 12\n",
            " Total batch num: 12\n",
            "----------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding image: 100%|██████████| 12/12 [00:20<00:00,  1.70s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Output feature path\": .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run by parameters from setting.json file\n",
        "\n",
        "If you want to using `settings.json` file which exported from Geo-SAM plugin to provide parameters, you can run it like this:"
      ],
      "metadata": {
        "id": "SuHYf5BQTT1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "setting_file = \"/content/setting.json\"\n",
        "feature_dir = './'\n",
        "\n",
        "### parse settings from the setting,json file\n",
        "settings = geosam.parse_settings_file(setting_file)\n",
        "\n",
        "### setting file not contains feature_dir, you need add it\n",
        "settings.update({\"feature_dir\":feature_dir})\n",
        "\n",
        "### split settings into init_settings, encode_settings\n",
        "init_settings, encode_settings = geosam.split_settings(settings)\n",
        "\n",
        "print(f\"settings: {settings}\")\n",
        "print(f\"init_settings: {init_settings}\")\n",
        "print(f\"encode_settings: {encode_settings}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4r3xTViM91YL",
        "outputId": "b8ea8757-24c8-467e-e68e-9539fe05b37a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "settings: {'image_path': '/content/beiluhe_google_img_201211_clip.tif', 'bands': [1, 1, 1], 'value_range': '0.0,255.0', 'extent': '471407.9709, 473331.8546, 3882162.2353, 3884389.1008 [EPSG:32646]', 'resolution': 0.9999395530145561, 'stride': 512, 'checkpoint_path': '/content/sam_vit_l_0b3195.pth', 'model_type': 1, 'batch_size': 1, 'gpu_id': 0, 'feature_dir': './'}\n",
            "init_settings: {'checkpoint_path': '/content/sam_vit_l_0b3195.pth', 'model_type': 1, 'batch_size': 1, 'gpu_id': 0}\n",
            "encode_settings: {'image_path': '/content/beiluhe_google_img_201211_clip.tif', 'bands': [1, 1, 1], 'value_range': '0.0,255.0', 'extent': '471407.9709, 473331.8546, 3882162.2353, 3884389.1008 [EPSG:32646]', 'resolution': 0.9999395530145561, 'stride': 512, 'feature_dir': './'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Then, you can run image encoding by parameters from setting.json file\n",
        "img_encoder = ImageEncoder(**init_settings)\n",
        "img_encoder.encode_image(**encode_settings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7fMTVTtFyjb",
        "outputId": "7178a14b-94e8-41bf-b623-eee0c4088d72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing SAM model...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------\n",
            "     Start encoding image to SAM features\n",
            "----------------------------------------------\n",
            "\n",
            "Input Parameters:\n",
            "----------------------------------------------\n",
            " Input data value range to be rescaled: (0.0, 255.0) (set by user)\n",
            " Image path: /content/beiluhe_google_img_201211_clip.tif\n",
            " Bands selected: ['1', '1', '1']\n",
            " Target resolution: 0.9999395530145561\n",
            " Processing extent: (471407.9709, 473331.8546, 3882162.2353, 3884389.1008)\n",
            " Processing image size: (width 3410960, height 3411263)\n",
            "----------------------------------------------\n",
            "\n",
            "\n",
            "RasterDataset info \n",
            "----------------------------------------------\n",
            " filename_glob: beiluhe_google_img_201211_clip.tif, \n",
            " all bands: ['1', '2', '3', '4'], \n",
            " input bands: ['1', '1', '1'], \n",
            " resolution: 0.9999395530145561, \n",
            " bounds: [471407.9709, 473331.8546, 3882162.2353, 3884389.1008, 0.0, 9.223372036854776e+18], \n",
            " image number: 1\n",
            "----------------------------------------------\n",
            "\n",
            "----------------------------------------------\n",
            " SAM model initialized. \n",
            "  SAM model type:  vit_l\n",
            " Device type: cuda:0\n",
            " Patch size: (1024, 1024) \n",
            " Batch size: 1\n",
            " Patch sample num: 12\n",
            " Total batch num: 12\n",
            "----------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding image: 100%|██████████| 12/12 [00:13<00:00,  1.08s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Output feature path\": .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Terminal\n",
        "\n",
        "Since this is a Colab example, Python will be used to demonstrate running it in the terminal."
      ],
      "metadata": {
        "id": "0YiFNWuz4iWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "## change cwd to geosam folder\n",
        "os.chdir(geosam.folder)\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDr4oyCKHMpV",
        "outputId": "51b3daad-df18-4a34-fe3a-cc0428b4ef94"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/geosam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## get the command for terminal\n",
        "cmd = f\"image_encoder.py -i {image_path} -c {checkpoint_path} -f {feature_dir}\"\n",
        "print(cmd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "vD686kzAXR6G",
        "outputId": "e05dc31f-33ff-4a86-ad4f-702368e96312"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'image_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-407ac606a545>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## get the command for terminal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"image_encoder.py -i {image_path} -c {checkpoint_path} -f {feature_dir}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'image_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## run in terminal\n",
        "!python image_encoder.py -i /content/beiluhe_google_img_201211_clip.tif -c /content/sam_vit_l_0b3195.pth -f ./\n",
        "\n",
        "## You can overwrite the settings from file by specify the parameter values. For Example:\n",
        "# !python image_encoder.py -s /content/setting.json  -f ./ --stride 256 --value_range \"10,255\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_LbTprzXa0M",
        "outputId": "7d264629-c1a5-4c89-feec-b11736831198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "settings:\n",
            " {'feature_dir': PosixPath('/usr/local/lib/python3.10/dist-packages/geosam'), 'image_path': PosixPath('/content/beiluhe_google_img_201211_clip.tif'), 'checkpoint_path': PosixPath('/content/sam_vit_l_0b3195.pth'), 'stride': 512, 'batch_size': 1, 'gpu_id': 0}\n",
            "\n",
            "Initializing SAM model...\n",
            "\n",
            "\n",
            "----------------------------------------------\n",
            "     Start encoding image to SAM features\n",
            "----------------------------------------------\n",
            "\n",
            "Input Parameters:\n",
            "----------------------------------------------\n",
            " Input data value range to be rescaled: [0, 255] (automatically set based on min-max value of input image inside the processing extent.)\n",
            " Image path: /content/beiluhe_google_img_201211_clip.tif\n",
            " Bands selected: ['1', '2', '3']\n",
            " Target resolution: 0.9999395530145561\n",
            " Processing extent: [471407.9709, 3882162.2353, 473331.8546, 3884389.1008]\n",
            " Processing image size: (width 1924, height 2227)\n",
            "----------------------------------------------\n",
            "\n",
            "\n",
            "RasterDataset info \n",
            "----------------------------------------------\n",
            " filename_glob: beiluhe_google_img_201211_clip.tif, \n",
            " all bands: ['1', '2', '3', '4'], \n",
            " input bands: ['1', '2', '3'], \n",
            " resolution: 0.9999395530145561, \n",
            " bounds: [471407.9709, 473331.8546571067, 3882162.2353493366, 3884389.1008, 0.0, 9.223372036854776e+18], \n",
            " num: 1\n",
            "----------------------------------------------\n",
            "\n",
            "----------------------------------------------\n",
            " SAM model initialized. \n",
            "  SAM model type:  vit_l\n",
            " Device type: cuda:0\n",
            " Patch size: (1024, 1024) \n",
            " Batch size: 1\n",
            " Patch sample num: 12\n",
            " Total batch num: 12\n",
            "----------------------------------------------\n",
            "\n",
            "Encoding image: 100% 12/12 [00:13<00:00,  1.16s/batch]\n",
            "\"Output feature path\": /usr/local/lib/python3.10/dist-packages/geosam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## check all available parameters:\n",
        "!python image_encoder.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aXYVNWVyUbg",
        "outputId": "5da59b2f-dc24-46fa-f6aa-b0bc6ae13097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "This script is for encoding image to SAM features.\n",
            "\n",
            "=====\n",
            "Usage\n",
            "=====\n",
            "using settings.json:\n",
            "\n",
            "    image_encoder.py -s <settings.json> -f <feature_dir>\n",
            " \n",
            " \n",
            "or directly using parameters:\n",
            " \n",
            "    image_encoder.py -i <image_path> -c <checkpoint_path> -f <feature_dir>\n",
            "    \n",
            "All Parameters:\n",
            "-------------------\n",
            "-s, --settings:         Path to the settings json file.\n",
            "-i, --image_path:       Path to the input image.\n",
            "-c, --checkpoint_path:  Path to the SAM checkpoint.\n",
            "-f, --feature_dir:      Path to the output feature directory.\n",
            "--model_type: one of [\"vit_h\", \"vit_l\", \"vit_b\"] or [0, 1, 2] or None, optional\n",
            "    The type of the SAM model. If None, the model type will be \n",
            "    inferred from the checkpoint path. Default: None. \n",
            "--bands: list of int, optional .\n",
            "    The bands to be used for encoding. Should not be more than three bands.\n",
            "    If None, the first three bands (if available) will be used. Default: None.\n",
            "--stride: int, optional\n",
            "    The stride of the sliding window. Default: 512.\n",
            "--extent: str, optional\n",
            "    The extent of the image to be encoded. Should be in the format of\n",
            "    \"minx, miny, maxx, maxy, [crs]\". If None, the extent of the input\n",
            "    image will be used. Default: None.\n",
            "--value_range: tuple of float, optional\n",
            "    The value range of the input image. If None, the value range will be\n",
            "    automatically calculated from the input image. Default: None.\n",
            "--resolution: float, optional\n",
            "    The resolution of the output feature in the unit of raster crs.\n",
            "    If None, the resolution of the input image will be used. Default: None.\n",
            "--batch_size: int, optional\n",
            "    The batch size for encoding. Default: 1.\n",
            "--gpu_id: int, optional\n",
            "    The device id of the GPU to be used. Default: 0.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}